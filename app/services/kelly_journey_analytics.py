"""
Kelly Journey Analytics Service

Customer journey mapping, visualization, drop-off analysis, journey optimization,
and pattern recognition with AI-powered insights and recommendations.
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import statistics
import numpy as np
from sqlalchemy import func, and_, or_, desc
from sqlalchemy.orm import Session

from app.database.connection import get_session
from app.models.kelly_crm import Contact, CustomerJourney, TouchPoint, Deal
from app.models.kelly_analytics import ConversationAnalytics
from app.models.kelly_intelligence import ConversationInsight, TopicAnalysis
from app.core.redis import redis_manager
from app.core.claude_ai import claude_client

logger = logging.getLogger(__name__)

@dataclass
class JourneyMap:
    """Customer journey map visualization data"""
    journey_id: str
    stages: List[Dict[str, Any]]
    touchpoints: List[Dict[str, Any]]
    key_moments: List[Dict[str, Any]]
    pain_points: List[Dict[str, Any]]
    opportunities: List[Dict[str, Any]]

@dataclass
class JourneyPattern:
    """Common journey pattern identification"""
    pattern_id: str
    pattern_name: str
    frequency: int
    success_rate: float
    typical_duration_days: float
    key_characteristics: List[str]
    outcomes: Dict[str, Any]

@dataclass
class DropOffAnalysis:
    """Journey drop-off point analysis"""
    drop_off_stage: str
    drop_off_rate: float
    common_reasons: List[str]
    risk_indicators: List[str]
    recovery_strategies: List[str]

@dataclass
class JourneyOptimization:
    """Journey optimization recommendations"""
    optimization_type: str
    priority: str
    expected_impact: str
    implementation_effort: str
    specific_actions: List[str]
    success_metrics: List[str]

class KellyJourneyAnalyticsService:
    """
    Advanced customer journey analytics service for Kelly AI system.
    
    Provides journey mapping, pattern recognition, drop-off analysis,
    and optimization recommendations with AI-powered insights.
    """
    
    def __init__(self):
        self.cache_ttl = 3600  # 1 hour cache for journey data
        self.journey_stages = ['awareness', 'consideration', 'decision', 'retention']
        self.typical_stage_durations = {
            'awareness': 7,      # 7 days
            'consideration': 14, # 14 days
            'decision': 21,      # 21 days
            'retention': 365     # 365 days
        }
    
    async def map_customer_journey(
        self,
        user_id: str,
        include_predictions: bool = True
    ) -> JourneyMap:
        """
        Create comprehensive customer journey map with AI insights.
        
        Args:
            user_id: Customer user identifier
            include_predictions: Whether to include AI predictions
            
        Returns:
            Complete journey map with visualization data
        """
        try:
            async with get_session() as session:
                # Get contact and journey
                contact = session.query(Contact).filter(Contact.user_id == user_id).first()
                if not contact:
                    raise ValueError(f\"Contact not found for user_id: {user_id}\")
                
                journey = session.query(CustomerJourney).filter(\n                    CustomerJourney.contact_id == contact.id\n                ).first()\n                \n                if not journey:\n                    raise ValueError(f\"Customer journey not found for user_id: {user_id}\")\n                \n                # Get all touchpoints\n                touchpoints = session.query(TouchPoint).filter(\n                    TouchPoint.contact_id == contact.id\n                ).order_by(TouchPoint.occurred_at.asc()).all()\n                \n                # Get conversations\n                conversations = session.query(ConversationAnalytics).filter(\n                    ConversationAnalytics.user_id == user_id\n                ).order_by(ConversationAnalytics.started_at.asc()).all()\n                \n                # Build stage progression\n                stages = await self._build_stage_progression(journey, touchpoints)\n                \n                # Map touchpoints to journey visualization\n                touchpoint_data = await self._map_touchpoints_to_journey(touchpoints, stages)\n                \n                # Identify key moments\n                key_moments = await self._identify_key_moments(conversations, touchpoints)\n                \n                # Identify pain points\n                pain_points = await self._identify_pain_points(touchpoints, conversations)\n                \n                # Identify opportunities\n                opportunities = await self._identify_opportunities(\n                    journey, touchpoints, conversations, include_predictions\n                )\n                \n                return JourneyMap(\n                    journey_id=str(journey.id),\n                    stages=stages,\n                    touchpoints=touchpoint_data,\n                    key_moments=key_moments,\n                    pain_points=pain_points,\n                    opportunities=opportunities\n                )\n                \n        except Exception as e:\n            logger.error(f\"Error mapping customer journey: {str(e)}\")\n            raise\n    \n    async def analyze_journey_patterns(\n        self,\n        account_id: Optional[str] = None,\n        time_period_days: int = 90,\n        min_pattern_frequency: int = 3\n    ) -> List[JourneyPattern]:\n        \"\"\"\n        Analyze common customer journey patterns using AI.\n        \n        Args:\n            account_id: Optional account filter\n            time_period_days: Analysis period in days\n            min_pattern_frequency: Minimum frequency for pattern recognition\n            \n        Returns:\n            List of identified journey patterns\n        \"\"\"\n        try:\n            cutoff_date = datetime.utcnow() - timedelta(days=time_period_days)\n            \n            async with get_session() as session:\n                # Get journeys for analysis\n                journeys_query = session.query(CustomerJourney).filter(\n                    CustomerJourney.journey_started_at >= cutoff_date\n                )\n                \n                if account_id:\n                    journeys_query = journeys_query.join(Contact).filter(\n                        Contact.user_id.like(f\"{account_id}%\")\n                    )\n                \n                journeys = journeys_query.all()\n                \n                if len(journeys) < min_pattern_frequency:\n                    return []\n                \n                # Group journeys by similar characteristics\n                pattern_groups = await self._group_journeys_by_patterns(journeys)\n                \n                # Analyze each pattern group\n                journey_patterns = []\n                for pattern_id, group_journeys in pattern_groups.items():\n                    if len(group_journeys) >= min_pattern_frequency:\n                        pattern = await self._analyze_journey_pattern(\n                            pattern_id, group_journeys\n                        )\n                        journey_patterns.append(pattern)\n                \n                # Sort by frequency and success rate\n                journey_patterns.sort(\n                    key=lambda p: (p.frequency, p.success_rate),\n                    reverse=True\n                )\n                \n                return journey_patterns\n                \n        except Exception as e:\n            logger.error(f\"Error analyzing journey patterns: {str(e)}\")\n            raise\n    \n    async def analyze_drop_offs(\n        self,\n        account_id: Optional[str] = None,\n        time_period_days: int = 90\n    ) -> List[DropOffAnalysis]:\n        \"\"\"\n        Analyze journey drop-off points and reasons using AI.\n        \n        Args:\n            account_id: Optional account filter\n            time_period_days: Analysis period in days\n            \n        Returns:\n            List of drop-off analysis results\n        \"\"\"\n        try:\n            cutoff_date = datetime.utcnow() - timedelta(days=time_period_days)\n            \n            async with get_session() as session:\n                # Get journeys that dropped off\n                journeys_query = session.query(CustomerJourney).filter(\n                    and_(\n                        CustomerJourney.journey_started_at >= cutoff_date,\n                        CustomerJourney.dropped_off == True\n                    )\n                )\n                \n                if account_id:\n                    journeys_query = journeys_query.join(Contact).filter(\n                        Contact.user_id.like(f\"{account_id}%\")\n                    )\n                \n                dropped_journeys = journeys_query.all()\n                \n                if not dropped_journeys:\n                    return []\n                \n                # Group by drop-off stage\n                stage_drop_offs = {}\n                for journey in dropped_journeys:\n                    stage = journey.drop_off_stage or journey.current_stage\n                    if stage not in stage_drop_offs:\n                        stage_drop_offs[stage] = []\n                    stage_drop_offs[stage].append(journey)\n                \n                # Analyze each stage's drop-offs\n                drop_off_analyses = []\n                for stage, stage_journeys in stage_drop_offs.items():\n                    analysis = await self._analyze_stage_drop_off(\n                        stage, stage_journeys, session\n                    )\n                    drop_off_analyses.append(analysis)\n                \n                # Sort by drop-off rate (highest first)\n                drop_off_analyses.sort(key=lambda a: a.drop_off_rate, reverse=True)\n                \n                return drop_off_analyses\n                \n        except Exception as e:\n            logger.error(f\"Error analyzing drop-offs: {str(e)}\")\n            raise\n    \n    async def generate_journey_optimizations(\n        self,\n        user_id: Optional[str] = None,\n        account_id: Optional[str] = None,\n        focus_areas: List[str] = None\n    ) -> List[JourneyOptimization]:\n        \"\"\"\n        Generate AI-powered journey optimization recommendations.\n        \n        Args:\n            user_id: Specific user filter\n            account_id: Account filter\n            focus_areas: Areas to focus optimization on\n            \n        Returns:\n            List of optimization recommendations\n        \"\"\"\n        try:\n            if not focus_areas:\n                focus_areas = ['conversion', 'engagement', 'retention', 'efficiency']\n            \n            async with get_session() as session:\n                optimizations = []\n                \n                # Analyze conversion optimization opportunities\n                if 'conversion' in focus_areas:\n                    conversion_opts = await self._generate_conversion_optimizations(\n                        session, user_id, account_id\n                    )\n                    optimizations.extend(conversion_opts)\n                \n                # Analyze engagement optimization opportunities\n                if 'engagement' in focus_areas:\n                    engagement_opts = await self._generate_engagement_optimizations(\n                        session, user_id, account_id\n                    )\n                    optimizations.extend(engagement_opts)\n                \n                # Analyze retention optimization opportunities\n                if 'retention' in focus_areas:\n                    retention_opts = await self._generate_retention_optimizations(\n                        session, user_id, account_id\n                    )\n                    optimizations.extend(retention_opts)\n                \n                # Analyze efficiency optimization opportunities\n                if 'efficiency' in focus_areas:\n                    efficiency_opts = await self._generate_efficiency_optimizations(\n                        session, user_id, account_id\n                    )\n                    optimizations.extend(efficiency_opts)\n                \n                # Sort by priority and expected impact\n                priority_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}\n                impact_order = {'high': 3, 'medium': 2, 'low': 1}\n                \n                optimizations.sort(\n                    key=lambda o: (priority_order.get(o.priority, 0), impact_order.get(o.expected_impact, 0)),\n                    reverse=True\n                )\n                \n                return optimizations[:10]  # Return top 10 optimizations\n                \n        except Exception as e:\n            logger.error(f\"Error generating journey optimizations: {str(e)}\")\n            raise\n    \n    async def get_journey_insights(\n        self,\n        user_id: str,\n        include_predictions: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Get comprehensive journey insights for a specific user.\n        \n        Args:\n            user_id: User identifier\n            include_predictions: Whether to include AI predictions\n            \n        Returns:\n            Comprehensive journey insights\n        \"\"\"\n        try:\n            async with get_session() as session:\n                # Get contact and journey\n                contact = session.query(Contact).filter(Contact.user_id == user_id).first()\n                if not contact:\n                    raise ValueError(f\"Contact not found for user_id: {user_id}\")\n                \n                journey = session.query(CustomerJourney).filter(\n                    CustomerJourney.contact_id == contact.id\n                ).first()\n                \n                if not journey:\n                    raise ValueError(f\"Customer journey not found for user_id: {user_id}\")\n                \n                # Get related data\n                touchpoints = session.query(TouchPoint).filter(\n                    TouchPoint.contact_id == contact.id\n                ).order_by(TouchPoint.occurred_at.desc()).limit(20).all()\n                \n                conversations = session.query(ConversationAnalytics).filter(\n                    ConversationAnalytics.user_id == user_id\n                ).order_by(ConversationAnalytics.started_at.desc()).limit(10).all()\n                \n                # Calculate journey metrics\n                journey_metrics = await self._calculate_journey_metrics(journey, touchpoints)\n                \n                # Analyze journey health\n                journey_health = await self._analyze_journey_health(journey, touchpoints, conversations)\n                \n                # Generate insights\n                insights = {\n                    'journey_overview': {\n                        'journey_id': str(journey.id),\n                        'current_stage': journey.current_stage,\n                        'journey_status': journey.journey_status,\n                        'started_at': journey.journey_started_at.isoformat(),\n                        'duration_days': (datetime.utcnow() - journey.journey_started_at).days,\n                        'total_touchpoints': journey.total_touchpoints\n                    },\n                    'journey_metrics': journey_metrics,\n                    'journey_health': journey_health,\n                    'stage_analysis': await self._analyze_stage_performance(journey),\n                    'engagement_analysis': await self._analyze_engagement_patterns(touchpoints),\n                    'conversation_impact': await self._analyze_conversation_impact(conversations)\n                }\n                \n                # Add predictions if requested\n                if include_predictions:\n                    predictions = await self._generate_journey_predictions(journey, touchpoints, conversations)\n                    insights['predictions'] = predictions\n                \n                return insights\n                \n        except Exception as e:\n            logger.error(f\"Error getting journey insights: {str(e)}\")\n            raise\n    \n    # Helper methods for journey analysis\n    \n    async def _build_stage_progression(self, journey: CustomerJourney, touchpoints: List[TouchPoint]) -> List[Dict[str, Any]]:\n        \"\"\"Build stage progression visualization data\"\"\"\n        stages = []\n        \n        for stage in self.journey_stages:\n            stage_data = {\n                'name': stage,\n                'status': 'completed' if self._stage_completed(journey, stage) else \n                         'current' if journey.current_stage == stage else 'pending',\n                'entered_at': self._get_stage_entered_date(journey, stage),\n                'duration_hours': self._get_stage_duration(journey, stage),\n                'touchpoint_count': len([tp for tp in touchpoints if tp.journey_stage == stage])\n            }\n            \n            # Add stage-specific metrics\n            if stage_data['status'] in ['completed', 'current']:\n                stage_touchpoints = [tp for tp in touchpoints if tp.journey_stage == stage]\n                if stage_touchpoints:\n                    stage_data['avg_engagement'] = statistics.mean([tp.engagement_quality for tp in stage_touchpoints])\n                    stage_data['sentiment_distribution'] = self._calculate_sentiment_distribution(stage_touchpoints)\n            \n            stages.append(stage_data)\n        \n        return stages\n    \n    async def _map_touchpoints_to_journey(self, touchpoints: List[TouchPoint], stages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Map touchpoints to journey visualization\"\"\"\n        touchpoint_data = []\n        \n        for tp in touchpoints:\n            touchpoint_info = {\n                'id': str(tp.id),\n                'type': tp.touchpoint_type,\n                'channel': tp.channel,\n                'direction': tp.direction,\n                'occurred_at': tp.occurred_at.isoformat(),\n                'journey_stage': tp.journey_stage,\n                'engagement_quality': tp.engagement_quality,\n                'sentiment': tp.sentiment,\n                'ai_handled': tp.ai_handled,\n                'summary': tp.summary\n            }\n            \n            # Add stage context\n            stage_info = next((s for s in stages if s['name'] == tp.journey_stage), None)\n            if stage_info:\n                touchpoint_info['stage_context'] = {\n                    'stage_status': stage_info['status'],\n                    'stage_progress': self._calculate_stage_progress(tp.occurred_at, stage_info)\n                }\n            \n            touchpoint_data.append(touchpoint_info)\n        \n        return touchpoint_data\n    \n    async def _identify_key_moments(self, conversations: List[ConversationAnalytics], touchpoints: List[TouchPoint]) -> List[Dict[str, Any]]:\n        \"\"\"Identify key moments in the customer journey\"\"\"\n        key_moments = []\n        \n        # High-quality conversations\n        high_quality_convs = [c for c in conversations if c.conversation_quality_score > 0.8]\n        for conv in high_quality_convs:\n            key_moments.append({\n                'type': 'high_quality_interaction',\n                'timestamp': conv.started_at.isoformat(),\n                'description': f\"High-quality conversation (score: {conv.conversation_quality_score:.2f})\",\n                'impact': 'positive',\n                'conversation_id': conv.conversation_id\n            })\n        \n        # Conversion events\n        conversion_convs = [c for c in conversations if c.conversion_event]\n        for conv in conversion_convs:\n            key_moments.append({\n                'type': 'conversion_event',\n                'timestamp': conv.started_at.isoformat(),\n                'description': \"Conversion event occurred\",\n                'impact': 'positive',\n                'conversation_id': conv.conversation_id\n            })\n        \n        # Negative sentiment clusters\n        negative_touchpoints = [tp for tp in touchpoints if tp.sentiment == 'negative']\n        if len(negative_touchpoints) >= 2:\n            # Group consecutive negative touchpoints\n            negative_clusters = self._group_consecutive_touchpoints(negative_touchpoints)\n            for cluster in negative_clusters:\n                if len(cluster) >= 2:\n                    key_moments.append({\n                        'type': 'negative_sentiment_cluster',\n                        'timestamp': cluster[0].occurred_at.isoformat(),\n                        'description': f\"Series of negative interactions ({len(cluster)} touchpoints)\",\n                        'impact': 'negative',\n                        'touchpoint_count': len(cluster)\n                    })\n        \n        # Sort by timestamp\n        key_moments.sort(key=lambda m: m['timestamp'])\n        \n        return key_moments\n    \n    async def _identify_pain_points(self, touchpoints: List[TouchPoint], conversations: List[ConversationAnalytics]) -> List[Dict[str, Any]]:\n        \"\"\"Identify pain points in the customer journey\"\"\"\n        pain_points = []\n        \n        # Low engagement touchpoints\n        low_engagement = [tp for tp in touchpoints if tp.engagement_quality < 0.3]\n        if len(low_engagement) > len(touchpoints) * 0.3:  # More than 30% low engagement\n            pain_points.append({\n                'type': 'low_engagement',\n                'severity': 'medium',\n                'description': f\"{len(low_engagement)} touchpoints with low engagement\",\n                'affected_touchpoints': len(low_engagement),\n                'recommendation': \"Review and improve engagement strategies\"\n            })\n        \n        # Long response times\n        conversations_with_slow_response = [c for c in conversations if c.avg_response_time_seconds and c.avg_response_time_seconds > 300]\n        if conversations_with_slow_response:\n            avg_slow_response = statistics.mean([c.avg_response_time_seconds for c in conversations_with_slow_response])\n            pain_points.append({\n                'type': 'slow_response_times',\n                'severity': 'high' if avg_slow_response > 600 else 'medium',\n                'description': f\"Average response time: {avg_slow_response/60:.1f} minutes\",\n                'affected_conversations': len(conversations_with_slow_response),\n                'recommendation': \"Optimize response time processes\"\n            })\n        \n        # Escalation patterns\n        escalated_conversations = [c for c in conversations if c.escalation_required]\n        if len(escalated_conversations) > len(conversations) * 0.2:  # More than 20% escalated\n            pain_points.append({\n                'type': 'high_escalation_rate',\n                'severity': 'high',\n                'description': f\"{len(escalated_conversations)} conversations required escalation\",\n                'escalation_rate': len(escalated_conversations) / len(conversations),\n                'recommendation': \"Review escalation triggers and improve AI training\"\n            })\n        \n        return pain_points\n    \n    async def _identify_opportunities(self, journey: CustomerJourney, touchpoints: List[TouchPoint], \n                                   conversations: List[ConversationAnalytics], include_predictions: bool) -> List[Dict[str, Any]]:\n        \"\"\"Identify opportunities for journey improvement\"\"\"\n        opportunities = []\n        \n        # High engagement opportunities\n        high_engagement_touchpoints = [tp for tp in touchpoints if tp.engagement_quality > 0.8]\n        if high_engagement_touchpoints:\n            successful_channels = {}\n            for tp in high_engagement_touchpoints:\n                successful_channels[tp.channel] = successful_channels.get(tp.channel, 0) + 1\n            \n            best_channel = max(successful_channels.items(), key=lambda x: x[1])\n            opportunities.append({\n                'type': 'channel_optimization',\n                'priority': 'medium',\n                'description': f\"Leverage {best_channel[0]} channel more (highest engagement)\",\n                'expected_impact': 'medium',\n                'success_rate': max([tp.engagement_quality for tp in high_engagement_touchpoints if tp.channel == best_channel[0]])\n            })\n        \n        # AI conversation opportunities\n        ai_conversations = [c for c in conversations if c.human_intervention_count == 0 and c.resolution_achieved]\n        if ai_conversations:\n            ai_success_rate = len(ai_conversations) / len(conversations)\n            if ai_success_rate > 0.7:\n                opportunities.append({\n                    'type': 'ai_expansion',\n                    'priority': 'high',\n                    'description': f\"Expand AI handling (current success rate: {ai_success_rate:.1%})\",\n                    'expected_impact': 'high',\n                    'potential_efficiency_gain': f\"{(1 - ai_success_rate) * 100:.1f}%\"\n                })\n        \n        # Stage progression opportunities\n        if journey.current_stage != 'retention':\n            next_stage_index = self.journey_stages.index(journey.current_stage) + 1\n            if next_stage_index < len(self.journey_stages):\n                next_stage = self.journey_stages[next_stage_index]\n                opportunities.append({\n                    'type': 'stage_progression',\n                    'priority': 'high',\n                    'description': f\"Ready to progress to {next_stage} stage\",\n                    'expected_impact': 'high',\n                    'recommended_actions': journey.next_best_actions or []\n                })\n        \n        # Predictive opportunities (if AI predictions enabled)\n        if include_predictions and journey.ai_recommendations:\n            for recommendation in journey.ai_recommendations.get('opportunities', []):\n                opportunities.append({\n                    'type': 'ai_predicted',\n                    'priority': recommendation.get('priority', 'medium'),\n                    'description': recommendation.get('description'),\n                    'expected_impact': recommendation.get('impact', 'medium'),\n                    'confidence': recommendation.get('confidence', 0.5)\n                })\n        \n        return opportunities\n    \n    def _stage_completed(self, journey: CustomerJourney, stage: str) -> bool:\n        \"\"\"Check if a journey stage is completed\"\"\"\n        stage_dates = {\n            'awareness': journey.awareness_entered_at,\n            'consideration': journey.consideration_entered_at,\n            'decision': journey.decision_entered_at,\n            'retention': journey.retention_entered_at\n        }\n        \n        return stage_dates.get(stage) is not None\n    \n    def _get_stage_entered_date(self, journey: CustomerJourney, stage: str) -> Optional[str]:\n        \"\"\"Get the date when a stage was entered\"\"\"\n        stage_dates = {\n            'awareness': journey.awareness_entered_at,\n            'consideration': journey.consideration_entered_at,\n            'decision': journey.decision_entered_at,\n            'retention': journey.retention_entered_at\n        }\n        \n        date = stage_dates.get(stage)\n        return date.isoformat() if date else None\n    \n    def _get_stage_duration(self, journey: CustomerJourney, stage: str) -> Optional[float]:\n        \"\"\"Get the duration spent in a stage\"\"\"\n        stage_durations = {\n            'awareness': journey.awareness_duration_hours,\n            'consideration': journey.consideration_duration_hours,\n            'decision': journey.decision_duration_hours,\n            'retention': journey.retention_duration_hours\n        }\n        \n        return stage_durations.get(stage)\n    \n    def _calculate_sentiment_distribution(self, touchpoints: List[TouchPoint]) -> Dict[str, int]:\n        \"\"\"Calculate sentiment distribution for touchpoints\"\"\"\n        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n        \n        for tp in touchpoints:\n            if tp.sentiment:\n                sentiment_counts[tp.sentiment] = sentiment_counts.get(tp.sentiment, 0) + 1\n        \n        return sentiment_counts\n    \n    def _calculate_stage_progress(self, touchpoint_time: datetime, stage_info: Dict[str, Any]) -> float:\n        \"\"\"Calculate progress within a stage based on touchpoint timing\"\"\"\n        if not stage_info.get('entered_at') or not stage_info.get('duration_hours'):\n            return 0.0\n        \n        stage_start = datetime.fromisoformat(stage_info['entered_at'])\n        stage_duration_hours = stage_info['duration_hours']\n        \n        if touchpoint_time < stage_start:\n            return 0.0\n        \n        hours_into_stage = (touchpoint_time - stage_start).total_seconds() / 3600\n        progress = min(hours_into_stage / stage_duration_hours, 1.0) if stage_duration_hours > 0 else 0.0\n        \n        return progress\n    \n    def _group_consecutive_touchpoints(self, touchpoints: List[TouchPoint]) -> List[List[TouchPoint]]:\n        \"\"\"Group consecutive touchpoints based on timing\"\"\"\n        if not touchpoints:\n            return []\n        \n        # Sort by occurrence time\n        sorted_touchpoints = sorted(touchpoints, key=lambda tp: tp.occurred_at)\n        \n        clusters = []\n        current_cluster = [sorted_touchpoints[0]]\n        \n        for i in range(1, len(sorted_touchpoints)):\n            current_tp = sorted_touchpoints[i]\n            prev_tp = sorted_touchpoints[i-1]\n            \n            # If touchpoints are within 24 hours, add to current cluster\n            time_diff = (current_tp.occurred_at - prev_tp.occurred_at).total_seconds() / 3600\n            if time_diff <= 24:\n                current_cluster.append(current_tp)\n            else:\n                clusters.append(current_cluster)\n                current_cluster = [current_tp]\n        \n        clusters.append(current_cluster)\n        return clusters\n    \n    async def _group_journeys_by_patterns(self, journeys: List[CustomerJourney]) -> Dict[str, List[CustomerJourney]]:\n        \"\"\"Group journeys by similar patterns\"\"\"\n        pattern_groups = {}\n        \n        for journey in journeys:\n            # Create pattern signature based on journey characteristics\n            pattern_signature = self._create_journey_pattern_signature(journey)\n            \n            if pattern_signature not in pattern_groups:\n                pattern_groups[pattern_signature] = []\n            pattern_groups[pattern_signature].append(journey)\n        \n        return pattern_groups\n    \n    def _create_journey_pattern_signature(self, journey: CustomerJourney) -> str:\n        \"\"\"Create a pattern signature for journey grouping\"\"\"\n        # Consider key characteristics for pattern matching\n        characteristics = []\n        \n        # Stage progression pattern\n        stages_reached = []\n        if journey.awareness_entered_at:\n            stages_reached.append('awareness')\n        if journey.consideration_entered_at:\n            stages_reached.append('consideration')\n        if journey.decision_entered_at:\n            stages_reached.append('decision')\n        if journey.retention_entered_at:\n            stages_reached.append('retention')\n        \n        characteristics.append(f\"stages:{'-'.join(stages_reached)}\")\n        \n        # Journey outcome\n        characteristics.append(f\"converted:{journey.converted}\")\n        characteristics.append(f\"dropped_off:{journey.dropped_off}\")\n        \n        # Engagement trend\n        if journey.engagement_trend:\n            characteristics.append(f\"engagement:{journey.engagement_trend}\")\n        \n        # Duration bucket\n        if journey.journey_started_at:\n            duration_days = (datetime.utcnow() - journey.journey_started_at).days\n            if duration_days <= 7:\n                duration_bucket = 'short'\n            elif duration_days <= 30:\n                duration_bucket = 'medium'\n            else:\n                duration_bucket = 'long'\n            characteristics.append(f\"duration:{duration_bucket}\")\n        \n        return \"|\".join(characteristics)\n    \n    async def _analyze_journey_pattern(self, pattern_id: str, journeys: List[CustomerJourney]) -> JourneyPattern:\n        \"\"\"Analyze a specific journey pattern\"\"\"\n        \n        # Calculate pattern metrics\n        frequency = len(journeys)\n        converted_journeys = [j for j in journeys if j.converted]\n        success_rate = len(converted_journeys) / frequency if frequency > 0 else 0\n        \n        # Calculate typical duration\n        durations = []\n        for journey in journeys:\n            if journey.journey_started_at:\n                if journey.actual_completion_date:\n                    duration = (journey.actual_completion_date - journey.journey_started_at).days\n                else:\n                    duration = (datetime.utcnow() - journey.journey_started_at).days\n                durations.append(duration)\n        \n        typical_duration = statistics.mean(durations) if durations else 0\n        \n        # Extract key characteristics\n        key_characteristics = self._extract_pattern_characteristics(journeys)\n        \n        # Analyze outcomes\n        outcomes = {\n            'conversion_rate': success_rate,\n            'avg_touchpoints': statistics.mean([j.total_touchpoints for j in journeys]),\n            'common_drop_off_stage': self._find_common_drop_off_stage(journeys),\n            'typical_engagement_trend': self._find_common_engagement_trend(journeys)\n        }\n        \n        # Generate pattern name\n        pattern_name = self._generate_pattern_name(pattern_id, key_characteristics, outcomes)\n        \n        return JourneyPattern(\n            pattern_id=pattern_id,\n            pattern_name=pattern_name,\n            frequency=frequency,\n            success_rate=success_rate,\n            typical_duration_days=typical_duration,\n            key_characteristics=key_characteristics,\n            outcomes=outcomes\n        )\n    \n    def _extract_pattern_characteristics(self, journeys: List[CustomerJourney]) -> List[str]:\n        \"\"\"Extract key characteristics from a group of journeys\"\"\"\n        characteristics = []\n        \n        # Most common current stage\n        stages = [j.current_stage for j in journeys if j.current_stage]\n        if stages:\n            most_common_stage = max(set(stages), key=stages.count)\n            characteristics.append(f\"Usually reaches {most_common_stage} stage\")\n        \n        # Engagement trend patterns\n        trends = [j.engagement_trend for j in journeys if j.engagement_trend]\n        if trends:\n            most_common_trend = max(set(trends), key=trends.count)\n            characteristics.append(f\"Engagement typically {most_common_trend}\")\n        \n        # Touchpoint volume patterns\n        avg_touchpoints = statistics.mean([j.total_touchpoints for j in journeys])\n        if avg_touchpoints < 5:\n            characteristics.append(\"Low touchpoint volume\")\n        elif avg_touchpoints > 15:\n            characteristics.append(\"High touchpoint volume\")\n        else:\n            characteristics.append(\"Moderate touchpoint volume\")\n        \n        return characteristics\n    \n    def _find_common_drop_off_stage(self, journeys: List[CustomerJourney]) -> Optional[str]:\n        \"\"\"Find the most common drop-off stage\"\"\"\n        drop_off_stages = [j.drop_off_stage for j in journeys if j.dropped_off and j.drop_off_stage]\n        if drop_off_stages:\n            return max(set(drop_off_stages), key=drop_off_stages.count)\n        return None\n    \n    def _find_common_engagement_trend(self, journeys: List[CustomerJourney]) -> Optional[str]:\n        \"\"\"Find the most common engagement trend\"\"\"\n        trends = [j.engagement_trend for j in journeys if j.engagement_trend]\n        if trends:\n            return max(set(trends), key=trends.count)\n        return None\n    \n    def _generate_pattern_name(self, pattern_id: str, characteristics: List[str], outcomes: Dict[str, Any]) -> str:\n        \"\"\"Generate a human-readable pattern name\"\"\"\n        \n        # Base name on success rate and key characteristics\n        success_rate = outcomes['conversion_rate']\n        \n        if success_rate > 0.8:\n            success_level = \"High-Success\"\n        elif success_rate > 0.5:\n            success_level = \"Moderate-Success\"\n        else:\n            success_level = \"Low-Success\"\n        \n        # Add distinctive characteristic\n        if \"Low touchpoint volume\" in characteristics:\n            pattern_type = \"Efficient\"\n        elif \"High touchpoint volume\" in characteristics:\n            pattern_type = \"Intensive\"\n        elif \"typically increasing\" in ' '.join(characteristics):\n            pattern_type = \"Accelerating\"\n        else:\n            pattern_type = \"Standard\"\n        \n        return f\"{success_level} {pattern_type} Journey\"\n    \n    async def _analyze_stage_drop_off(self, stage: str, journeys: List[CustomerJourney], session: Session) -> DropOffAnalysis:\n        \"\"\"Analyze drop-offs at a specific stage\"\"\"\n        \n        # Calculate drop-off rate for this stage\n        total_reached_stage = len([j for j in journeys if self._stage_completed(j, stage)])\n        dropped_at_stage = len(journeys)\n        \n        drop_off_rate = (dropped_at_stage / total_reached_stage * 100) if total_reached_stage > 0 else 0\n        \n        # Analyze common reasons (would use AI analysis of touchpoints/conversations)\n        common_reasons = await self._identify_drop_off_reasons(stage, journeys, session)\n        \n        # Identify risk indicators\n        risk_indicators = await self._identify_drop_off_risk_indicators(stage, journeys)\n        \n        # Generate recovery strategies\n        recovery_strategies = await self._generate_drop_off_recovery_strategies(stage, common_reasons)\n        \n        return DropOffAnalysis(\n            drop_off_stage=stage,\n            drop_off_rate=drop_off_rate,\n            common_reasons=common_reasons,\n            risk_indicators=risk_indicators,\n            recovery_strategies=recovery_strategies\n        )\n    \n    async def _identify_drop_off_reasons(self, stage: str, journeys: List[CustomerJourney], session: Session) -> List[str]:\n        \"\"\"Identify common reasons for drop-offs at a stage\"\"\"\n        reasons = []\n        \n        # Analyze journey metadata for drop-off reasons\n        for journey in journeys:\n            if journey.drop_off_reason:\n                reasons.append(journey.drop_off_reason)\n        \n        # If no explicit reasons, infer from patterns\n        if not reasons:\n            if stage == 'awareness':\n                reasons = [\"Low initial interest\", \"Poor value proposition clarity\", \"Timing not right\"]\n            elif stage == 'consideration':\n                reasons = [\"Product-market fit concerns\", \"Price sensitivity\", \"Competitive alternatives\"]\n            elif stage == 'decision':\n                reasons = [\"Budget constraints\", \"Decision timeline mismatch\", \"Risk concerns\"]\n            else:\n                reasons = [\"Unmet expectations\", \"Service quality issues\", \"Changed requirements\"]\n        \n        return list(set(reasons))[:5]  # Return unique reasons, max 5\n    \n    async def _identify_drop_off_risk_indicators(self, stage: str, journeys: List[CustomerJourney]) -> List[str]:\n        \"\"\"Identify risk indicators that predict drop-offs\"\"\"\n        risk_indicators = []\n        \n        # Common risk indicators by stage\n        stage_risks = {\n            'awareness': [\n                \"Low engagement in first 48 hours\",\n                \"No follow-up after initial contact\",\n                \"Negative sentiment in early interactions\"\n            ],\n            'consideration': [\n                \"Extended time without progression\",\n                \"Declining engagement quality\",\n                \"Competitor mentions increasing\"\n            ],\n            'decision': [\n                \"Delayed responses to proposals\",\n                \"New stakeholder involvement\",\n                \"Budget approval delays\"\n            ],\n            'retention': [\n                \"Decreased usage patterns\",\n                \"Support ticket frequency increase\",\n                \"Contract renewal delays\"\n            ]\n        }\n        \n        return stage_risks.get(stage, [\"Unidentified risk factors\"])\n    \n    async def _generate_drop_off_recovery_strategies(self, stage: str, common_reasons: List[str]) -> List[str]:\n        \"\"\"Generate recovery strategies for drop-offs\"\"\"\n        strategies = []\n        \n        # Reason-specific strategies\n        reason_strategies = {\n            \"Low initial interest\": \"Improve initial value proposition and engagement\",\n            \"Price sensitivity\": \"Provide clear ROI demonstration and flexible pricing options\",\n            \"Competitive alternatives\": \"Strengthen competitive differentiation and unique value\",\n            \"Budget constraints\": \"Offer phased implementation or trial periods\",\n            \"Timing not right\": \"Implement nurture campaigns for future engagement\"\n        }\n        \n        for reason in common_reasons:\n            if reason in reason_strategies:\n                strategies.append(reason_strategies[reason])\n        \n        # Stage-specific general strategies\n        stage_strategies = {\n            'awareness': \"Enhance content marketing and initial touchpoint quality\",\n            'consideration': \"Provide comprehensive product demonstrations and case studies\",\n            'decision': \"Streamline decision process and address objections proactively\",\n            'retention': \"Implement proactive success management and value reinforcement\"\n        }\n        \n        if stage in stage_strategies:\n            strategies.append(stage_strategies[stage])\n        \n        return list(set(strategies))  # Return unique strategies\n    \n    async def _generate_conversion_optimizations(self, session: Session, user_id: Optional[str], account_id: Optional[str]) -> List[JourneyOptimization]:\n        \"\"\"Generate conversion-focused optimizations\"\"\"\n        optimizations = []\n        \n        # Example conversion optimization\n        optimizations.append(JourneyOptimization(\n            optimization_type=\"conversion\",\n            priority=\"high\",\n            expected_impact=\"high\",\n            implementation_effort=\"medium\",\n            specific_actions=[\n                \"Implement progressive profiling in consideration stage\",\n                \"Add social proof elements to decision touchpoints\",\n                \"Create urgency through limited-time offers\"\n            ],\n            success_metrics=[\n                \"Conversion rate increase by 15%\",\n                \"Reduced time in decision stage by 20%\",\n                \"Improved lead quality scores\"\n            ]\n        ))\n        \n        return optimizations\n    \n    async def _generate_engagement_optimizations(self, session: Session, user_id: Optional[str], account_id: Optional[str]) -> List[JourneyOptimization]:\n        \"\"\"Generate engagement-focused optimizations\"\"\"\n        optimizations = []\n        \n        optimizations.append(JourneyOptimization(\n            optimization_type=\"engagement\",\n            priority=\"medium\",\n            expected_impact=\"medium\",\n            implementation_effort=\"low\",\n            specific_actions=[\n                \"Personalize content based on stage and preferences\",\n                \"Implement multi-channel engagement sequences\",\n                \"Add interactive elements to touchpoints\"\n            ],\n            success_metrics=[\n                \"Engagement quality score increase by 25%\",\n                \"Reduced bounce rate in early stages\",\n                \"Increased touchpoint frequency\"\n            ]\n        ))\n        \n        return optimizations\n    \n    async def _generate_retention_optimizations(self, session: Session, user_id: Optional[str], account_id: Optional[str]) -> List[JourneyOptimization]:\n        \"\"\"Generate retention-focused optimizations\"\"\"\n        optimizations = []\n        \n        optimizations.append(JourneyOptimization(\n            optimization_type=\"retention\",\n            priority=\"high\",\n            expected_impact=\"high\",\n            implementation_effort=\"high\",\n            specific_actions=[\n                \"Implement predictive churn modeling\",\n                \"Create proactive success management programs\",\n                \"Develop value reinforcement campaigns\"\n            ],\n            success_metrics=[\n                \"Churn rate reduction by 30%\",\n                \"Increased customer lifetime value\",\n                \"Improved satisfaction scores\"\n            ]\n        ))\n        \n        return optimizations\n    \n    async def _generate_efficiency_optimizations(self, session: Session, user_id: Optional[str], account_id: Optional[str]) -> List[JourneyOptimization]:\n        \"\"\"Generate efficiency-focused optimizations\"\"\"\n        optimizations = []\n        \n        optimizations.append(JourneyOptimization(\n            optimization_type=\"efficiency\",\n            priority=\"medium\",\n            expected_impact=\"medium\",\n            implementation_effort=\"medium\",\n            specific_actions=[\n                \"Automate routine touchpoints with AI\",\n                \"Implement smart routing based on intent\",\n                \"Create self-service options for common needs\"\n            ],\n            success_metrics=[\n                \"Reduced time-to-conversion by 25%\",\n                \"Decreased manual touchpoint requirements\",\n                \"Improved AI handling rate\"\n            ]\n        ))\n        \n        return optimizations\n    \n    async def _calculate_journey_metrics(self, journey: CustomerJourney, touchpoints: List[TouchPoint]) -> Dict[str, Any]:\n        \"\"\"Calculate comprehensive journey metrics\"\"\"\n        \n        if not touchpoints:\n            return {'error': 'No touchpoints available for analysis'}\n        \n        # Basic metrics\n        total_touchpoints = len(touchpoints)\n        avg_engagement = statistics.mean([tp.engagement_quality for tp in touchpoints])\n        \n        # Sentiment distribution\n        sentiment_counts = self._calculate_sentiment_distribution(touchpoints)\n        \n        # Channel distribution\n        channel_counts = {}\n        for tp in touchpoints:\n            channel_counts[tp.channel] = channel_counts.get(tp.channel, 0) + 1\n        \n        # Timing metrics\n        touchpoint_intervals = []\n        sorted_touchpoints = sorted(touchpoints, key=lambda tp: tp.occurred_at)\n        for i in range(1, len(sorted_touchpoints)):\n            interval = (sorted_touchpoints[i].occurred_at - sorted_touchpoints[i-1].occurred_at).total_seconds() / 3600\n            touchpoint_intervals.append(interval)\n        \n        avg_interval_hours = statistics.mean(touchpoint_intervals) if touchpoint_intervals else 0\n        \n        return {\n            'total_touchpoints': total_touchpoints,\n            'avg_engagement_quality': round(avg_engagement, 3),\n            'sentiment_distribution': sentiment_counts,\n            'channel_distribution': channel_counts,\n            'avg_touchpoint_interval_hours': round(avg_interval_hours, 2),\n            'journey_velocity_score': min(100, max(0, 100 - avg_interval_hours))  # Lower intervals = higher velocity\n        }\n    \n    async def _analyze_journey_health(self, journey: CustomerJourney, touchpoints: List[TouchPoint], \n                                    conversations: List[ConversationAnalytics]) -> Dict[str, Any]:\n        \"\"\"Analyze overall journey health\"\"\"\n        \n        health_factors = []\n        health_score = 75  # Start with neutral score\n        \n        # Engagement health\n        if touchpoints:\n            recent_touchpoints = [tp for tp in touchpoints if (datetime.utcnow() - tp.occurred_at).days <= 14]\n            if recent_touchpoints:\n                avg_recent_engagement = statistics.mean([tp.engagement_quality for tp in recent_touchpoints])\n                if avg_recent_engagement > 0.7:\n                    health_factors.append(\"High recent engagement\")\n                    health_score += 10\n                elif avg_recent_engagement < 0.3:\n                    health_factors.append(\"Low recent engagement\")\n                    health_score -= 15\n        \n        # Progression health\n        if journey.engagement_trend == 'increasing':\n            health_factors.append(\"Positive engagement trend\")\n            health_score += 10\n        elif journey.engagement_trend == 'decreasing':\n            health_factors.append(\"Declining engagement trend\")\n            health_score -= 10\n        \n        # Conversation quality health\n        if conversations:\n            avg_quality = statistics.mean([c.conversation_quality_score for c in conversations])\n            if avg_quality > 0.8:\n                health_factors.append(\"High conversation quality\")\n                health_score += 15\n            elif avg_quality < 0.5:\n                health_factors.append(\"Poor conversation quality\")\n                health_score -= 15\n        \n        # Journey progression health\n        journey_duration_days = (datetime.utcnow() - journey.journey_started_at).days\n        expected_duration = sum(self.typical_stage_durations.values())\n        \n        if journey_duration_days > expected_duration * 1.5:\n            health_factors.append(\"Extended journey duration\")\n            health_score -= 10\n        elif journey_duration_days < expected_duration * 0.5:\n            health_factors.append(\"Rapid journey progression\")\n            health_score += 5\n        \n        # Determine overall health status\n        if health_score >= 85:\n            health_status = 'excellent'\n        elif health_score >= 70:\n            health_status = 'good'\n        elif health_score >= 50:\n            health_status = 'fair'\n        else:\n            health_status = 'poor'\n        \n        return {\n            'health_score': max(0, min(100, health_score)),\n            'health_status': health_status,\n            'health_factors': health_factors,\n            'risk_level': 'high' if health_score < 50 else 'medium' if health_score < 70 else 'low'\n        }\n    \n    async def _analyze_stage_performance(self, journey: CustomerJourney) -> Dict[str, Any]:\n        \"\"\"Analyze performance in each journey stage\"\"\"\n        \n        stage_performance = {}\n        \n        for stage in self.journey_stages:\n            stage_data = {\n                'completed': self._stage_completed(journey, stage),\n                'current': journey.current_stage == stage\n            }\n            \n            if stage_data['completed'] or stage_data['current']:\n                duration = self._get_stage_duration(journey, stage)\n                typical_duration = self.typical_stage_durations.get(stage, 14) * 24  # Convert to hours\n                \n                if duration:\n                    stage_data['duration_hours'] = duration\n                    stage_data['performance'] = 'fast' if duration < typical_duration * 0.7 else \\\n                                              'slow' if duration > typical_duration * 1.3 else 'normal'\n                    stage_data['efficiency_score'] = min(100, max(0, (typical_duration / duration) * 100))\n            \n            stage_performance[stage] = stage_data\n        \n        return stage_performance\n    \n    async def _analyze_engagement_patterns(self, touchpoints: List[TouchPoint]) -> Dict[str, Any]:\n        \"\"\"Analyze engagement patterns across touchpoints\"\"\"\n        \n        if not touchpoints:\n            return {'error': 'No touchpoints for analysis'}\n        \n        # Sort touchpoints by time\n        sorted_touchpoints = sorted(touchpoints, key=lambda tp: tp.occurred_at)\n        \n        # Analyze engagement over time\n        engagement_over_time = [tp.engagement_quality for tp in sorted_touchpoints]\n        \n        # Calculate trend\n        if len(engagement_over_time) >= 3:\n            x_values = list(range(len(engagement_over_time)))\n            slope = np.polyfit(x_values, engagement_over_time, 1)[0]\n            trend = 'increasing' if slope > 0.05 else 'decreasing' if slope < -0.05 else 'stable'\n        else:\n            trend = 'insufficient_data'\n        \n        # Analyze by channel\n        channel_engagement = {}\n        for tp in touchpoints:\n            if tp.channel not in channel_engagement:\n                channel_engagement[tp.channel] = []\n            channel_engagement[tp.channel].append(tp.engagement_quality)\n        \n        # Calculate channel performance\n        channel_performance = {}\n        for channel, engagements in channel_engagement.items():\n            channel_performance[channel] = {\n                'avg_engagement': statistics.mean(engagements),\n                'touchpoint_count': len(engagements),\n                'consistency': 1 - statistics.stdev(engagements) if len(engagements) > 1 else 1\n            }\n        \n        return {\n            'engagement_trend': trend,\n            'avg_engagement': statistics.mean(engagement_over_time),\n            'engagement_volatility': statistics.stdev(engagement_over_time) if len(engagement_over_time) > 1 else 0,\n            'channel_performance': channel_performance,\n            'peak_engagement': max(engagement_over_time),\n            'lowest_engagement': min(engagement_over_time)\n        }\n    \n    async def _analyze_conversation_impact(self, conversations: List[ConversationAnalytics]) -> Dict[str, Any]:\n        \"\"\"Analyze the impact of conversations on journey progression\"\"\"\n        \n        if not conversations:\n            return {'error': 'No conversations for analysis'}\n        \n        # Quality metrics\n        quality_scores = [c.conversation_quality_score for c in conversations]\n        avg_quality = statistics.mean(quality_scores)\n        \n        # Outcome metrics\n        total_conversations = len(conversations)\n        resolved_conversations = len([c for c in conversations if c.resolution_achieved])\n        escalated_conversations = len([c for c in conversations if c.escalation_required])\n        conversion_conversations = len([c for c in conversations if c.conversion_event])\n        \n        # AI performance\n        ai_only_conversations = len([c for c in conversations if c.human_intervention_count == 0])\n        ai_success_rate = len([c for c in conversations if c.human_intervention_count == 0 and c.resolution_achieved]) / max(ai_only_conversations, 1)\n        \n        # Impact analysis\n        high_impact_conversations = [c for c in conversations if c.conversation_quality_score > 0.8 or c.conversion_event]\n        \n        return {\n            'total_conversations': total_conversations,\n            'avg_quality_score': round(avg_quality, 3),\n            'resolution_rate': round(resolved_conversations / total_conversations, 3),\n            'escalation_rate': round(escalated_conversations / total_conversations, 3),\n            'conversion_rate': round(conversion_conversations / total_conversations, 3),\n            'ai_success_rate': round(ai_success_rate, 3),\n            'high_impact_conversations': len(high_impact_conversations),\n            'conversation_efficiency': round((resolved_conversations + conversion_conversations) / total_conversations, 3)\n        }\n    \n    async def _generate_journey_predictions(self, journey: CustomerJourney, touchpoints: List[TouchPoint], \n                                          conversations: List[ConversationAnalytics]) -> Dict[str, Any]:\n        \"\"\"Generate AI-powered journey predictions\"\"\"\n        \n        predictions = {}\n        \n        # Next stage prediction\n        if journey.current_stage != 'retention':\n            current_stage_index = self.journey_stages.index(journey.current_stage)\n            if current_stage_index < len(self.journey_stages) - 1:\n                next_stage = self.journey_stages[current_stage_index + 1]\n                \n                # Simple prediction based on engagement and progress\n                recent_engagement = 0.5\n                if touchpoints:\n                    recent_touchpoints = [tp for tp in touchpoints if (datetime.utcnow() - tp.occurred_at).days <= 7]\n                    if recent_touchpoints:\n                        recent_engagement = statistics.mean([tp.engagement_quality for tp in recent_touchpoints])\n                \n                progression_probability = min(0.9, max(0.1, recent_engagement + 0.2))\n                \n                predictions['next_stage'] = {\n                    'stage': next_stage,\n                    'probability': round(progression_probability, 3),\n                    'estimated_timeline_days': self.typical_stage_durations.get(journey.current_stage, 14)\n                }\n        \n        # Conversion prediction\n        if journey.current_stage in ['consideration', 'decision']:\n            # Simple conversion prediction based on quality and engagement\n            quality_factor = 0.5\n            if conversations:\n                avg_quality = statistics.mean([c.conversation_quality_score for c in conversations])\n                quality_factor = avg_quality\n            \n            engagement_factor = 0.5\n            if touchpoints:\n                avg_engagement = statistics.mean([tp.engagement_quality for tp in touchpoints])\n                engagement_factor = avg_engagement\n            \n            conversion_probability = (quality_factor + engagement_factor) / 2\n            \n            predictions['conversion'] = {\n                'probability': round(conversion_probability, 3),\n                'confidence': 0.7,  # Static confidence for now\n                'key_factors': ['conversation_quality', 'engagement_level', 'stage_progression']\n            }\n        \n        # Churn risk prediction\n        churn_risk_factors = []\n        churn_score = 0.1  # Low base risk\n        \n        # Check for negative indicators\n        if touchpoints:\n            recent_touchpoints = [tp for tp in touchpoints if (datetime.utcnow() - tp.occurred_at).days <= 14]\n            if not recent_touchpoints:\n                churn_risk_factors.append('No recent activity')\n                churn_score += 0.3\n            else:\n                negative_sentiment = len([tp for tp in recent_touchpoints if tp.sentiment == 'negative'])\n                if negative_sentiment > len(recent_touchpoints) / 2:\n                    churn_risk_factors.append('Majority negative sentiment')\n                    churn_score += 0.4\n        \n        if journey.engagement_trend == 'decreasing':\n            churn_risk_factors.append('Declining engagement trend')\n            churn_score += 0.2\n        \n        predictions['churn_risk'] = {\n            'risk_score': min(1.0, churn_score),\n            'risk_level': 'high' if churn_score > 0.7 else 'medium' if churn_score > 0.4 else 'low',\n            'risk_factors': churn_risk_factors\n        }\n        \n        return predictions\n\n# Create global journey analytics service instance\nkelly_journey_analytics = KellyJourneyAnalyticsService()"