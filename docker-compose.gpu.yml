# GPU-Enabled Docker Compose Configuration
# For AI/ML workloads requiring CUDA acceleration

version: '3.8'

services:
  # ==================== GPU-Enabled Application ====================
  app-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
      target: gpu-production
    container_name: ai-conversation-app-gpu
    restart: unless-stopped
    runtime: nvidia
    environment:
      # GPU Configuration
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      CUDA_VISIBLE_DEVICES: 0
      
      # Application
      ENVIRONMENT: production
      DEBUG: false
      LOG_LEVEL: INFO
      HOST: 0.0.0.0
      PORT: 8000
      WORKERS: 1
      
      # Database
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: ${DB_NAME:-ai_conversation_gpu}
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      DB_POOL_SIZE: 20
      DB_MAX_OVERFLOW: 30
      
      # Redis
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_MAX_CONNECTIONS: 50
      
      # Security
      SECRET_KEY: ${SECRET_KEY}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      
      # Telegram\n      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN}\n      TELEGRAM_WEBHOOK_URL: ${TELEGRAM_WEBHOOK_URL}\n      \n      # ML Configuration - GPU Optimized\n      ML_DEVICE: cuda\n      ML_MODEL_PATH: /app/models\n      ML_MODEL_CACHE_SIZE: 3\n      ML_ENABLE_GPU: true\n      ML_BATCH_SIZE: 64\n      TORCH_CUDA_ARCH_LIST: \"6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0\"\n      \n      # Celery\n      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1\n      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2\n      \n      # Monitoring\n      SENTRY_DSN: ${SENTRY_DSN}\n      SENTRY_ENVIRONMENT: production-gpu\n      METRICS_ENABLED: true\n      METRICS_PORT: 8001\n      \n      # Advanced Typing with GPU acceleration\n      ENABLE_ADVANCED_TYPING: true\n      MAX_TYPING_SESSIONS: 200\n      ENABLE_PERSONALITY_TYPING: true\n    ports:\n      - \"8000:8000\"\n      - \"8001:8001\"\n    volumes:\n      - ./models:/app/models\n      - ./logs:/app/logs\n      - ./data:/app/data\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 8G\n        reservations:\n          cpus: '2.0'\n          memory: 4G\n        # GPU reservations (requires Docker Swarm or Kubernetes)\n        generic_resources:\n          - discrete_resource_spec:\n              kind: 'NVIDIA-GPU'\n              value: 1\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 15s\n      retries: 3\n      start_period: 120s  # Longer startup time for GPU initialization\n\n  # ==================== GPU ML Workers ====================\n  worker-ml-gpu-1:\n    build:\n      context: .\n      dockerfile: Dockerfile.gpu\n      target: gpu-worker\n    container_name: ai-conversation-worker-ml-gpu-1\n    restart: unless-stopped\n    runtime: nvidia\n    command: [\"celery\", \"-A\", \"app.worker\", \"worker\", \"--loglevel=info\", \"--concurrency=1\", \"--prefetch-multiplier=1\", \"--queue=ml_gpu_tasks\"]\n    environment:\n      # GPU Configuration\n      NVIDIA_VISIBLE_DEVICES: 0\n      NVIDIA_DRIVER_CAPABILITIES: compute,utility\n      CUDA_VISIBLE_DEVICES: 0\n      \n      ENVIRONMENT: production\n      LOG_LEVEL: INFO\n      \n      # Database\n      DB_HOST: postgres\n      DB_PORT: 5432\n      DB_NAME: ${DB_NAME:-ai_conversation_gpu}\n      DB_USER: ${DB_USER}\n      DB_PASSWORD: ${DB_PASSWORD}\n      \n      # Redis\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n      \n      # ML Configuration - GPU Optimized\n      ML_DEVICE: cuda\n      ML_MODEL_PATH: /app/models\n      ML_MODEL_CACHE_SIZE: 2\n      ML_ENABLE_GPU: true\n      ML_BATCH_SIZE: 32\n      \n      # Celery\n      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1\n      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2\n      CELERY_WORKER_CONCURRENCY: 1  # Lower concurrency for GPU memory\n      \n      # Monitoring\n      SENTRY_DSN: ${SENTRY_DSN}\n      SENTRY_ENVIRONMENT: production-gpu\n    volumes:\n      - ./models:/app/models\n      - ./logs:/app/logs\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 12G\n        reservations:\n          cpus: '2.0'\n          memory: 6G\n        generic_resources:\n          - discrete_resource_spec:\n              kind: 'NVIDIA-GPU'\n              value: 1\n\n  worker-ml-gpu-2:\n    build:\n      context: .\n      dockerfile: Dockerfile.gpu\n      target: gpu-worker\n    container_name: ai-conversation-worker-ml-gpu-2\n    restart: unless-stopped\n    runtime: nvidia\n    command: [\"celery\", \"-A\", \"app.worker\", \"worker\", \"--loglevel=info\", \"--concurrency=1\", \"--prefetch-multiplier=1\", \"--queue=ml_gpu_tasks\"]\n    environment:\n      # GPU Configuration\n      NVIDIA_VISIBLE_DEVICES: 1\n      NVIDIA_DRIVER_CAPABILITIES: compute,utility\n      CUDA_VISIBLE_DEVICES: 0  # Worker sees as device 0\n      \n      ENVIRONMENT: production\n      LOG_LEVEL: INFO\n      \n      # Database\n      DB_HOST: postgres\n      DB_PORT: 5432\n      DB_NAME: ${DB_NAME:-ai_conversation_gpu}\n      DB_USER: ${DB_USER}\n      DB_PASSWORD: ${DB_PASSWORD}\n      \n      # Redis\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n      \n      # ML Configuration - GPU Optimized\n      ML_DEVICE: cuda\n      ML_MODEL_PATH: /app/models\n      ML_MODEL_CACHE_SIZE: 2\n      ML_ENABLE_GPU: true\n      ML_BATCH_SIZE: 32\n      \n      # Celery\n      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1\n      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2\n      CELERY_WORKER_CONCURRENCY: 1\n      \n      # Monitoring\n      SENTRY_DSN: ${SENTRY_DSN}\n      SENTRY_ENVIRONMENT: production-gpu\n    volumes:\n      - ./models:/app/models\n      - ./logs:/app/logs\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 12G\n        reservations:\n          cpus: '2.0'\n          memory: 6G\n        generic_resources:\n          - discrete_resource_spec:\n              kind: 'NVIDIA-GPU'\n              value: 1\n    profiles:\n      - multi-gpu  # Only start with multi-GPU profile\n\n  # ==================== Regular CPU Workers ====================\n  worker-general:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      target: worker\n    container_name: ai-conversation-worker-general-gpu\n    restart: unless-stopped\n    command: [\"celery\", \"-A\", \"app.worker\", \"worker\", \"--loglevel=info\", \"--concurrency=4\", \"--prefetch-multiplier=1\", \"--queue=general,default\"]\n    environment:\n      ENVIRONMENT: production\n      LOG_LEVEL: INFO\n      \n      # Database\n      DB_HOST: postgres\n      DB_PORT: 5432\n      DB_NAME: ${DB_NAME:-ai_conversation_gpu}\n      DB_USER: ${DB_USER}\n      DB_PASSWORD: ${DB_PASSWORD}\n      \n      # Redis\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n      \n      # Celery\n      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1\n      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2\n      CELERY_WORKER_CONCURRENCY: 4\n      \n      # Monitoring\n      SENTRY_DSN: ${SENTRY_DSN}\n      SENTRY_ENVIRONMENT: production-gpu\n    volumes:\n      - ./logs:/app/logs\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '2.0'\n          memory: 2G\n        reservations:\n          cpus: '1.0'\n          memory: 1G\n\n  # ==================== Scheduler ====================\n  scheduler:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      target: scheduler\n    container_name: ai-conversation-scheduler-gpu\n    restart: unless-stopped\n    environment:\n      ENVIRONMENT: production\n      LOG_LEVEL: INFO\n      \n      # Database\n      DB_HOST: postgres\n      DB_PORT: 5432\n      DB_NAME: ${DB_NAME:-ai_conversation_gpu}\n      DB_USER: ${DB_USER}\n      DB_PASSWORD: ${DB_PASSWORD}\n      \n      # Redis\n      REDIS_HOST: redis\n      REDIS_PORT: 6379\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n      \n      # Celery\n      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/1\n      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/2\n      \n      # Monitoring\n      SENTRY_DSN: ${SENTRY_DSN}\n      SENTRY_ENVIRONMENT: production-gpu\n    volumes:\n      - ./logs:/app/logs\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 1G\n        reservations:\n          cpus: '0.5'\n          memory: 512M\n\n  # ==================== Database ====================\n  postgres:\n    image: postgres:15-alpine\n    container_name: ai-conversation-postgres-gpu\n    restart: unless-stopped\n    environment:\n      POSTGRES_DB: ${DB_NAME:-ai_conversation_gpu}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_INITDB_ARGS: \"--encoding=UTF-8 --lc-collate=C --lc-ctype=C\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_gpu_data:/var/lib/postgresql/data\n      - ./postgres/init-scripts:/docker-entrypoint-initdb.d\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '2.0'\n          memory: 4G\n        reservations:\n          cpus: '1.0'\n          memory: 2G\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER} -d ${DB_NAME}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  # ==================== Redis ====================\n  redis:\n    image: redis:7-alpine\n    container_name: ai-conversation-redis-gpu\n    restart: unless-stopped\n    command: redis-server --requirepass ${REDIS_PASSWORD} --maxmemory 4gb --maxmemory-policy allkeys-lru --appendonly yes\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_gpu_data:/data\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '2.0'\n          memory: 5G\n        reservations:\n          cpus: '1.0'\n          memory: 4G\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"-a\", \"${REDIS_PASSWORD}\", \"ping\"]\n      interval: 10s\n      timeout: 3s\n      retries: 5\n\n  # ==================== GPU Monitoring ====================\n  gpu-monitoring:\n    image: nvidia/dcgm-exporter:3.1.8-3.1.5-ubuntu20.04\n    container_name: ai-conversation-gpu-exporter\n    restart: unless-stopped\n    runtime: nvidia\n    environment:\n      NVIDIA_VISIBLE_DEVICES: all\n      NVIDIA_DRIVER_CAPABILITIES: compute,utility\n    ports:\n      - \"9400:9400\"\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n    cap_add:\n      - SYS_ADMIN\n    profiles:\n      - monitoring\n\n  # ==================== Prometheus for GPU Metrics ====================\n  prometheus-gpu:\n    image: prom/prometheus:v2.47.0\n    container_name: ai-conversation-prometheus-gpu\n    restart: unless-stopped\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./monitoring/prometheus/prometheus-gpu.yml:/etc/prometheus/prometheus.yml:ro\n      - prometheus_gpu_data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention.time=15d'\n      - '--web.console.libraries=/usr/share/prometheus/console_libraries'\n      - '--web.console.templates=/usr/share/prometheus/consoles'\n      - '--web.enable-lifecycle'\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 2G\n        reservations:\n          cpus: '0.5'\n          memory: 1G\n    profiles:\n      - monitoring\n\n  # ==================== Grafana for GPU Dashboards ====================\n  grafana-gpu:\n    image: grafana/grafana:10.1.0\n    container_name: ai-conversation-grafana-gpu\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin123}\n      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}\n      GF_INSTALL_PLUGINS: grafana-piechart-panel,grafana-worldmap-panel\n    volumes:\n      - grafana_gpu_data:/var/lib/grafana\n      - ./monitoring/grafana/provisioning-gpu:/etc/grafana/provisioning:ro\n      - ./monitoring/grafana/dashboards-gpu:/var/lib/grafana/dashboards:ro\n    depends_on:\n      - prometheus-gpu\n    networks:\n      - ai-gpu-network\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 1G\n        reservations:\n          cpus: '0.5'\n          memory: 512M\n    profiles:\n      - monitoring\n\n# ==================== Volume Definitions ====================\nvolumes:\n  postgres_gpu_data:\n    driver: local\n  redis_gpu_data:\n    driver: local\n  prometheus_gpu_data:\n    driver: local\n  grafana_gpu_data:\n    driver: local\n\n# ==================== Network Definitions ====================\nnetworks:\n  ai-gpu-network:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.25.0.0/16